services:
#  mentor-backend:
#    build:
#      context: .
#      dockerfile: Dockerfile.backend
#    container_name: mentor-backend
#    restart: unless-stopped
#    ports:
#      - "8083:8083"
#    depends_on:
#      ollama:
#        condition: service_healthy
#    environment:
#      SPRING_PROFILES_ACTIVE: docker
#      JAVA_OPTS: "-Xms512m -Xmx1024m"

#  mentor-frontend:
#    build:
#      context: .
#      dockerfile: Dockerfile.frontend
#    container_name: mentor-frontend
#    restart: unless-stopped
#    ports:
#      - "5173:4173"
#    depends_on:
#      - mentor-backend


  ollama:
    image: docker.io/ollama/ollama:latest
    container_name: mentor-ollama
    restart: unless-stopped
    entrypoint: ["/usr/bin/bash", "/ollama-init.sh"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities:
                - gpu
    ports:
      - "11434:11434"
    environment:
      PRELOAD_MODEL_NAME: hf.co/unsloth/gemma-3n-E4B-it-GGUF:Q4_K_XL
      CUDA_VISIBLE_DEVICES: 0
      OLLAMA_MODELS: /root/.ollama/models
      OLLAMA_KEEP_ALIVE: -1
      OLLAMA_FLASH_ATTENTION: 1
      OLLAMA_CONTEXT_LENGTH: 8192
      #OLLAMA_KV_CACHE_TYPE: q4_0
      OLLAMA_DEBUG: 1
    volumes:
      - ollama-data:/root/.ollama
      - ./ollama-init.sh:/ollama-init.sh
    healthcheck:
      test: ["CMD", "ollama", "--version"]
      interval: 1h
      timeout: 5s
      retries: 1
      start_period: 10s

volumes:
  ollama-data: